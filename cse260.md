## Cache组相联

### 直接映射

一个内存地址能映射到的cache line是固定的。

优点：硬件简单，成本低，地址变换速度快，且不涉及替换算法问题

缺点：不够灵活，cache的存储空间得不到充分利用，每个主存块只有一个固定位置存放，容易产生冲突。

### 全相联映射

主存中的一个地址可被映射到任意cache line。

优点：比较灵活，cache的利用率高，块冲突概率低，只要淘汰cache中的某一块，即可调入主存的任一块。

缺点：当寻找一个地址是否已经被cache时，需要遍历每一个cache line来寻找。

### 组相联映射

主存和cache都分组，主存中一个组内的块数与cache的分组数相同，组间采用直接映射，组内采用全相联映射。主存中的各块与cache的组号之间有固定的映射关系，但可自由映射到对应cache组内的任何一块。

## Reuse Patterns (Cache Locality)
* Temporal -- something we used recently we will reuse again
* Spatial -- something we used will be near something else we will use (e.g. structure)

## The 3 C's of Cache Miss
* Cold Miss
  * first time we are using the data
* Capacity Miss
  * Not enough room
  * Cache just not big enough
* Conflict Miss
  * Cache might be big enough, and we might have seen the data before but...
    * Too many set aliases

## Computational Intensity

Performance is limited by the **ratio** of computation done and the volume of data moved between processor and main memory.

We call this ratio the computational intensity `q`. 

$q=\dfrac{f}{m}=\dfrac{\text{counting multiply and adds}}{\text{counting loads and stores}}$

unit of q: ops/memop

$$\text{Predicted time}=ft_f(1+\dfrac{1}{t_f/t_m}\dfrac{1}{q})$$

## Roofline Model

* Peak performance intrinsic to the processor, theoretical limit based on clock speed
* Lower rooflines correspond to successively degraded performance as we remove optimizations
* If hardware has a multiplier and an adder, but algorithm can use only adders, then we can do no better than 1/2 peak
* Similarly with SIMD parallelism and ILP
* More bandwidth means steeper slope
* Corner $q=\dfrac{Peak GFLOPs}{Bandwidth}$

## Instruction Level Parallelism

SIMD -- Single Instruction, Multiple Data

### Obstacles to vectorization
* Control Divergence
  * What if not all loop iterations are the same?
  * Facilities to deal with control divergence
    * Blending and masking
* Data Dependencies
  * Loop carried dependency

### Alignment

* Aligned 128-bit -- can get data in one access
* Unaligned 128-bit -- may need two different column addresses so may need multiple cycles
* Cache line crossing -- need to access two different rows -- may need multiple cycles

## Address Space Classifications

* Global Memory
  * Multi-Processor
  * Partitioned Global Address Space
* Non Shared
  * Multicomputer (Cluster)

## Multiprocessor Organization
* Address space is global to all processors
* Hardware chooses policy for mapping address space to processor
* UMA
  * Uniform Memory Access
  * Symmetric Multiprocessors
* NUMA
  * Non-Uniform Memory Access
  * Pretty much all multiprocessors more than a single device

### Architectures w/o Shared Memory
Data exchanged through messages
* HPC cluster using MPI
* Hadoop or Spark Cluster
* Special purpose clusters (e.g. WebTier, database tier)

## Multithreading
* Shared memory systems
* Each thread has own stack, registers
* Threads communicate via shared memory (e.g. heap access)
* Coordinate via shared variable and other sychronization primitives

## OpenMP
Add pragmas to allow compiler to parallelize (source code annotations)

Examples:
``` C
#pragma omp parallel private(i) shared(n)

#pragma omp for
for (i = 0; i < n; i++)
    work(i);
```
The above program can be transformed to
``` C
i0 = $TID * n / $nthreads;
i1 = i0 + n/$nthreads;
for (i = i0; i < i1; i++)
    work(i);
```

``` C
#pragma omp parallel for
for (i = 0; i < N; i++)
    A[i] = B[i] + C[i];
```

### Workload decomposition
* Translator automatically generates local loop bounds (`#pragma omp for`)
* We use private/shared pragmas to distinguish thread private from global variable
* Decomposition can be static or dynamic
* Dynamic assignment for irregular problems

### OMP parallel for
* OpenMP can parallize loops that are in canonical form
  * `for (idx = start; idx </<=/>=/> end; idx++/++idx/idx--/--idx/idx+=inc/idx-=inc)`
* Cannot exit loop early with conditional (e.g. `if (j) break;`)
* Loop iterations must be independent
  * No loop carried dependencies
  * Programmer must check for these

### Loop Dependence Analysis
* OpenMP cannot parallelize a loop unless it is free to reorder the iterations arbitrarily
* True dependence:
  * X <- Y
  * Z <- X
* Output dependence
  * X <- Y
  * X <- Z
* Anti dependence
  * X <- Y
  * Y <- Z
* OpenMP has implicit barrier between two loops unless NOWAIT is specified
  
### Loop Carried Dependency
* An index is affine if of the form `a * i + b`
* Assume loops have bounds `m` and `n`. `j` and `k` are within `m` and `n`.
* If loop
  * Stores to an array at `a * j + b`
  * Later reads from the array at `c * k + d`
* If a, b, c, d are known at compile time, then compiler can apply GCD test
  * `(d - b) % GCD(c, a) == 0`: greatest common denominator evenly divides `d - b`
  * Necessary but not sufficient test for conflict -- test does not consider loop bounds

### Data Race
* Critical sections (mutex)
  * Only one thread can be in the critical region at a time
  * Serialize a section of code
* Atomic variables
  * Must be one of the following types
    * `x <op>= <expressions>, x++, ++x, x--, --x`
    * Expression cannot reference x
  * Only applies to one statement
  * May only apply to part of a statemetn
  * May map to atomic primitives in the instruction set for high performance
* Barriers

## GPU

### Sorken's K80 GPU
* K80 - 2 GPUs/PCIe card, compute capability 3.7, each with
  * 13 streaming processors @ 875 MHz
  * Peak performance: 2.91 Tflops/s Double Precision, fused multiply/add
  * 128K 32 bit registers
* SIMT parallelism
* 24 GB device memory @ 480 GB/s
* SMX Processor
  * 192 SP Core, 64 DP Cores, 32 SFUs, 32 Ld/St Units
* 1 FMA/cycle = 2 flops/cycle/DP core * 64 DP/SMX * 13 = 1664 flops/cycle
* x2 G210 @ 875 MHz = 2.91 TFlop/sec

### Thread Hierarchy
* **Thread** is the smallest work unit, each thread is executed in a **Streaming Processor** (SIMD lane)
* Treads are grouped into **thread blocks**. A thread block is the minimum dispatch and retirement unit, and it will be allocated to a **Streaming Multiprocessors** (SMs)
* A **grid** contains multiple blocks, a **kernel** is executed on a grid.

### Kernel Execution
* Each thread is executed by a core (SMX)
* Each block is executed by one SM
* Several concurrent block can reside in one SM depending on the block's memory requirements and SM's memory resources
* Each kernel is executed on one device
* Multiple kernels can execute on a device at one time

### Thread Execution Model
* Kernel call spawns virtualized, hierarchically organized threads
  * Grid -> Block -> Thread
* Hardware dispatches thread block to SMX (SMs)
* SMs schedule threads to run on cores in groups called **warps**
  * 0 overhead switching between warps
* Compiler re-arranges loads to hide latencies --  hardware schedules between warps to hide latency
* Global synchronization: kernel invocation

### Thread Block Execution
* Thread blocks
  * Unit of workload assignment
  * Own set of registers
  * Access to a fast on-chip shared memory
  * Synchronization among threads in block
  * Different blocks sync/communicate via slow global memory
  * Processor groups threads into warps of 32 threads
* SIMT: all threads in a warp execute the same instruction
  * All branches followed
  * Instructions disabled
  * Divergence, serialization

### Thread Blocks to Warp Mapping
* Thread block is 1, 2, or 3-D
* These are linearized to one dimension which is assigned to warps
* Warps are assigned in groups 32 threads (until less than 32 threads remain) from the linearized arrangement
  * Have same start PC, but each thread keeps track of its own PC
  * Best performance when all threads maintain the same PC

  
### Warp Scheduling
* Threads assigned to an SMX in units of a thread block, multiple blocks assigned to SMX
* Each block divided into warps of 32 threads, a schedulable unit
  * warp becomes eligible for execution when all operands available
  * Dynamic instruction reordering: eligible warps selected for execution using a prioritized scheduling policy
  * All threads in a Warp execute the same instruction, differing branches serialize execution
* Multiple warps simultaneously active, hiding data transfer delays
* Hardware is free to assign blocks to any SMX

### CUDA Hierarchy
* All threads in a block execute the same sequence of instructions -- SPMD
* Thread id = `blockIdx.x * blockDim.x + threadIdx.x`

### Types of Storage
* Automatic variables (non array)
  * Registers
  * Thread private
* Automatic variables (array)
  * Local memory (may be stored in device memory)
  * Thread private
* Shared memory
  * `__device__ __shared__`
  * Can be shared within the thread block
* Global memory
  * `__device__ __global__`
  * Can be shared by grid
* Constant memory
  * `__device__ __constant__`
  * Can be shared by grid
  * Stored in global but can be cached for speed

### Occupancy
* Occupancy = # of active warps / max # of warps supported by SM
* Max number limited by SM resources, function of:
  * Shared memory
  * Registers
  * Block size

### GPU Memory System
#### Memory Terminology
* Channels
  * Controllers
* Ranks
  * DRAM that share a bus
  * GDDR (only one rank)
* Banks
  * Inside DRAM
  * Several banks open
  * Pay latency once, stream multiple banks
  * Avoid bank conflicts
#### Interleaving
* GPUs and CPUs generally have multiple memory channels and banks
  * Addresses are interleaved between the channels
  * Sequential and hashing schemes

**TODO: understand 0204 and 0206 PPT!!!!!!!!**

**全然わからない**

### Kepler's Memory Hierarchy
* L2 cache are shared by SMX
* If two thread blocks from different SMX's load same memory, then L2 cache may hit
* Reduce # of loads to GDDR DRAM, more efficient use of bandwidth

## Instruction Level Parallelism
* Kepler
  * 255 32-bit registers per thread
  * 64K 32-bit registers total (per SMX)
* Kepler is dual issue
  * 2 instructions / warp / cycle, up to 4 warps at a time
  * A warp executes 32 threads, all with the same instruction
  * 2 instruction/warp is the same as 2 instructions/thread * 32 threads in a warp

## Bandwidth, Throughput and Latency
### Bandwidth
Bandwidth is a measure of how much data over time a communication link can handle, its capacity. This is typically measured as bps (bits per second)

### Throughput
Throughput is the actual amount of data that is successfully sent/received over the communication link. Throughput is presented as bps, and can differ from bandwidth due to a range of technical issues, including latency, packet loss, jitter and more.

### Latency
Latency is the time it taks for a packet to get across the network, from source to destination. It is measured in units of time.

## Little's Law
* L = $\lambda$ W
  * Concurrent units = arrival rate * service time
  * Concurrency (parallelism) = throughput x latency
  * Memory instructions = memory-BW x memory-latency
  * Arithmetic instructions = arithmetic-throughput x arithmetic-latency
* Latency = end time - start time
* Throughput = # instructions completed / interval
* Concurrency = average # of instructions in execution

### Instruction and Op concurrency for arithmetic
* Kepler has 192 single precision cores/SMX
* 1 instruction does 32 operations (one warp)
  * 192 / 32 = 6 SP instructions/cycle to maximize throughput
* Instruction latency = 9 cycles
* Instruction concurrency (peak instruction-level parallelism) via Little's Law = 9 cycles * 6 IPC = 54 instructions

### Instruction Concurrency for Memory Operations
* Concurrency = memory latency x memory bandwidth
* Latency = 368 cycles, 16 SMs, each load coalesced does 128 B, latency 386 cycles
* Pin bandwidth = 211 GB/sec (measured)
* IPC = 211 GB/sec * 1/(1.266 GHz) * 1/(16 SM) * instruction / (128B) = 0.081 IPC/SM - throughput
* Required parallelism to hide memory latency
  * Concurrency = 386 cycles * 0.081 IPC/SM = 31 instructions in flight / SM

## Achieve Max Arithmetic Throughput

### Thread Level Parallelism
Disadvantage: more overhead at sync barriers (especially in cases of thread divergence, cache variability, etc.)

### Instruction Level Parallelism
Do more per thread with less threads

## Required occupancy to hide mem latency
* $\alpha$ is computational intensity (ratio of ops/mem)
  * $\alpha=0$ all memory ops (memory bound)
  * $\alpha=\infty$ all arithmetic ops (computation bound)
* Required warps are not dissimilar
  * Good correlation for arithmetic workload
  * Memory saturation effects with memory workload
* Why Cusp Behavior
  * 3 limits to concurrency
    * Active warps (occupancy) = warp latency * warp throughput
    * Occ = latency * min(mem\_thru, alu\_thru/$\alpha$, issue\_thru/$\alpha$)
    * Latency = (mem\_lat + $\alpha$ alu\_lat)
    * Mem\_latency -- one memory operation per code snippet
    * Alu\_lat -- $\alpha$ arithmetic ops per code snippet
  * Execute one warp at a time, then it takes $\alpha$ alu\_throughput + memlatency
  * Small $\alpha$ = memory bound
    * N = (mem_lat + $\alpha$ alu_lat) mem_thru
    * N = mem_lat mem_thru + $\alpha$ alu_lat mem_thru
    * N increases as $\alpha$ increases
  * Large $\alpha$ = alu bound (or issue_thru bound)
    * N = (mem_lat + $\alpha$ alu_lat) alu_thru / $\alpha$
    * N = mem_lat alu_thru / $\alpha$ + alu_lat alu_thru
    * N decreases as $\alpha$ increases
  * **Max** when $\alpha=\dfrac{alu_{thru}}{mem_{thru}}$
  * N(max) = alu_lat alu_thru + mem_lat mem_thru

## Performance Prediction
* Computation Time -- similar to Amdahl's Law
  * Total work nodes W(n)
  * Work nodes on critial Path D(n)
* Completion time is parallel portion + serial portion
  * T_comp = D(n) + (W(n) - D(n)) / p
* Predict Sorken Performance for n=512
  * data_motion = 2 * 512^3 * 8 Byte / (240 GB/sec) = 8.95 ms
  * compute_time = 2 * 512^3 / (1.37 TFlops) = 195 uSec
  * The application is communication bound
* Tile Analysis: q = O(L*sqrt(M_fast))

## CUDA Implementation of Prediction
* All instructions support prediction
* Condition code or predicate per thread: set to true or false
* Execute only if the predicate is true, else NOP
* Compiler replaces a branch instruction with predicated instructions only if instructions guarded by branch condition is not too large (threshold)
* Cost of prediction relative to branch

## Hierarchical Instruction Scheduling
* Thread blocks are scheduled to SMX
* Within SMX -- warps are scheduled to execution units (cores)

## Kepler's Thread Scheduling
* 4 warp scheduler's
  * 2 instruction dispatch per warp scheduler
  * Each warp scheduler can dispatch every cycle
* Thread block
  * Entire thread block assigned to SMX
  * Stays on SMX until complete
  * Max 2048 threads / SMX
  * Max 1024 threads / block
  * Max 16 blocks / SMX
  * Warps of 32 threads
* Warp scheduler
  * SM scheduler chooses instruction from available warps
  * Scoreboard can keep track of which warps are ready to issue
  * Warps can be from different blocks or sam block but out of order
  * Switch between warps (in the same block) with 0 overhead
  
**TODO: read 0213/0218/0220 PPT**

## Stencil methods
* Many physical problems simulated on uniform mesh in 1, 2, or 3 dimensions
* Field variables defined on a discrete set of points

## Types of Cluster Programming Models
* Client-Server -- messages passed over a network protocol like UDP or TCP
* Remote Procedure Call
* Map Reduce -- data passed through files over a network (on top of some network protocol)
* MPI -- message passing interface

## Bulk Synchronous Execution Model
* 3 phases (superstep)
  * Compute
  * Communicate 
  * Synchronize
* L is superstep interval
* Processes communicate h messages per superstep
* g is communication cost (1/BW + fixed overhead)
* T_comm = gh (ignoring message startup cost)

## Asynchronous, non-blocking Communication
* Phase 1: intiate communication with the immediate 'I' variant of point-to-point call `MPI_Irecv()`, `MPI_Isend()`
* Phase 2: synchronize `MPI_Wait()`
* Must sync w/ wait before reusing send buffer or accessing recv buffer
* Every pending `IRecv()` must have a distinct buffer
* Can do compute on other stuff between `Isend()`/`Irecv()` and Wait

## Message Passing
* LogP model
  * L -- latency
  * o -- overhead
  * g -- gap between messages
  * P -- processors
* $\alpha$ $\beta$ model
  * Message passing time = $\alpha+\beta^{-1}_{\infty}n$
  * $\alpha$ -- message latency
  * $\beta$ -- peak bandwidth (bytes per second)
  * n -- message length
  * Short messages: latency term dominates
  * Long messages: bandwidth term dominates 
* Half Power Point
  * T(n) -- time to transmit a packet of size n
  * $\beta$(n) is effective bandwidth = n / T(n)
  * $n_{1/2}$ is n that achieves $1/2\beta_\infty=n_{1/2} / T(n_{1/2})$
  * Theory says that this occurs at $\alpha=\beta_{\infty}^{-1}n_{1/2}$
    * $n_{1/2}=\alpha\beta_\infty$

## Performance Measurement
* Completion Time -- how long did it take?
* Processor Time Product -- completion time x # processors
* Throughput -- amount of work that can be completed in some unit of time
* Relative Performance -- compare to a reference (speedup)
* Scalability -- how does the execution time relate to increasing resources

### Parallel Speedup and Efficiency and Fiction
* Sp = completion time of the fasted single threaded program (Ts) / completion time of the parallel program (Tp)
* Efficiency = Ep = Sp / p

### Strong vs Weak Scaling
* Strong Scaling -- when the problem size remains fixed
* Weak Scaling -- problem size (N) increases with P

### Computing Scaled Speedup
* PTp = T1 + To
  * T1 is the best serial program time
  * To is parallel overhead
* To is a function of P
* speedup = T1 / Tp = PT1 / (T1 + To)

### Isoefficiency
* Increase N (work) with P -- to get weak scaling
* N is expressed in operations (not data size)
* N = f(p) is linear in P, computation is scalable
* S = T1 / Tp
* E = S / P = T1 / (T1 + To)